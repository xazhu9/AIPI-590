{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xazhu9/AIPI-590/blob/main/Asignment2%20patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to github"
      ],
      "metadata": {
        "id": "Llf-YIo080PD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kFkkhoC-XEk",
        "outputId": "7b29a5d9-2afc-4cf2-f032-45677b996b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AIPI-590' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/xazhu9/AIPI-590.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWHmLMWf_RiV"
      },
      "source": [
        "FGSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbP-4kXB_fvQ",
        "outputId": "cd665072-5d27-4343-d3c4-ebbb86b725ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Loss: 0.2478: 100%|██████████| 141/141 [00:52<00:00,  2.68it/s]\n",
            "Epoch 2, Loss: 0.3070:  13%|█▎        | 18/141 [00:06<00:37,  3.30it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# load model and dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # change graph size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(\"/content/AIPI-590/TinyImageNet\", transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# load ResNet34 model\n",
        "model = models.resnet34(weights='IMAGENET1K_V1')\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "TENSOR_MEANS = torch.FloatTensor([0.485, 0.456, 0.406])[:, None, None].to(device)\n",
        "TENSOR_STD = torch.FloatTensor([0.229, 0.224, 0.225])[:, None, None].to(device)\n",
        "\n",
        "def patch_forward(patch):\n",
        "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
        "    return patch\n",
        "\n",
        "def place_patch(img, patch):\n",
        "    for i in range(img.shape[0]):\n",
        "        h_offset = np.random.randint(0, img.shape[2] - patch.shape[1])\n",
        "        w_offset = np.random.randint(0, img.shape[3] - patch.shape[2])\n",
        "        img[i, :, h_offset:h_offset+patch.shape[1], w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
        "    return img\n",
        "\n",
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader):\n",
        "            for _ in range(4):\n",
        "                patch_img = place_patch(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp / counter\n",
        "    top5 = tp_5 / counter\n",
        "    return acc, top5\n",
        "\n",
        "# train the patch\n",
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size, patch_size, device=device), requires_grad=True)\n",
        "    optimizer = optim.SGD([patch], lr=0.1, momentum=0.9)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader)\n",
        "        for img, _ in t:\n",
        "            img = place_patch(img, patch)\n",
        "            img = img.to(device)\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}\n",
        "\n",
        "# train patch with toaster\n",
        "target_class = 859  # the toaster class\n",
        "patch, results = patch_attack(model, target_class, patch_size=64, num_epochs=5)\n",
        "\n",
        "print(\"Patch training completed. Accuracy: \", results[\"acc\"], \"Top-5 Accuracy: \", results[\"top5\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show comparison between with and without patch"
      ],
      "metadata": {
        "id": "f4NOfOg66g2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# transform to visable graph\n",
        "def tensor_to_image(tensor):\n",
        "    tensor = tensor.squeeze().cpu().detach()\n",
        "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    tensor = torch.clip(tensor, 0, 1)\n",
        "    return tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "# get class identification result\n",
        "def get_prediction(model, image):\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0).to(device))\n",
        "    pred_class = output.argmax(dim=1).item()\n",
        "    return pred_class\n",
        "\n",
        "# show the graph and class comparison\n",
        "def display_comparison_with_predictions(original_imgs, patched_imgs, model, patch, class_names, num_images=5):\n",
        "    plt.figure(figsize=(15, num_images * 5))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # get the class for graphs with and without patch\n",
        "        original_pred = get_prediction(model, original_imgs[i])\n",
        "        patched_pred = get_prediction(model, patched_imgs[i])\n",
        "\n",
        "        # original graph\n",
        "        plt.subplot(num_images, 3, i * 3 + 1)\n",
        "        plt.title(f\"Original Image (Pred: {class_names[original_pred]})\")\n",
        "        plt.imshow(tensor_to_image(original_imgs[i]))\n",
        "        plt.axis('off')\n",
        "\n",
        "        # graph with patch\n",
        "        plt.subplot(num_images, 3, i * 3 + 2)\n",
        "        plt.title(f\"Patched Image (Pred: {class_names[patched_pred]})\")\n",
        "        plt.imshow(tensor_to_image(patched_imgs[i]))\n",
        "        plt.axis('off')\n",
        "\n",
        "        # show patch\n",
        "        if i == 0:\n",
        "            patch_img = (torch.tanh(patch) + 1) / 2\n",
        "            patch_img = patch_img.cpu().detach().numpy()\n",
        "            patch_img = np.clip(patch_img, 0, 1)\n",
        "\n",
        "            plt.subplot(num_images, 3, i * 3 + 3)\n",
        "            plt.title(f\"Adversarial Patch\")\n",
        "            plt.imshow(np.transpose(patch_img, (1, 2, 0)))\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# load the class\n",
        "with open(\"/content/AIPI-590/imagenet_classes.txt\") as f:\n",
        "    class_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# select a series of graph\n",
        "images, labels = next(iter(data_loader))\n",
        "images = images.to(device)\n",
        "\n",
        "# generate graph with patch\n",
        "patched_images = place_patch(images.clone(), patch)\n",
        "\n",
        "# Show the comparison for graphs and class\n",
        "display_comparison_with_predictions(images, patched_images, model, patch, class_names, num_images=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "yflk5k_G0FvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the patch"
      ],
      "metadata": {
        "id": "fsncR65O2IQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def save_patch_as_image(patch, file_name=\"adversarial_patch.png\"):\n",
        "    # transform to numpy\n",
        "    patch_img = (torch.tanh(patch) + 1) / 2\n",
        "    patch_img = patch_img.squeeze().cpu().detach().numpy()\n",
        "    patch_img = np.clip(patch_img, 0, 1)\n",
        "\n",
        "    # transform\n",
        "    patch_img = np.transpose(patch_img, (1, 2, 0))\n",
        "    patch_img = (patch_img * 255).astype(np.uint8)\n",
        "\n",
        "    # Save the image\n",
        "    img = Image.fromarray(patch_img)\n",
        "    img.save(file_name)\n",
        "    print(f\"Patch saved as {file_name}\")\n",
        "\n",
        "# Save a png file\n",
        "save_patch_as_image(patch, \"adversarial_patch.png\")\n",
        "from google.colab import files\n",
        "files.download(\"/content/adversarial_patch.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_4QOcpIp2LOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0lgD3tP5usugCjLCPMHZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}